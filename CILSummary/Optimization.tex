% !TEX root = Main.tex
\section{Optimization}

\subsection*{Alternating Least Squares}
$f(U,v_i)=\sum_{(i,j)\in I} (a_{i,j} - \langle u_j, v_i \rangle)^2$\\
$f(u_i,V)=\sum_{(i,j)\in I} (a_{i,j} - \langle u_j, v_i \rangle)^2$\\
Least squares problems which are convex.


1. init: $\mathbf{x}^{(0)} \in \mathbb{R}^D$\\
2. for $t = 0 \ \text{to} \ \mathit{maxIter}$:\\
3. $\mathbf{x}^{(t+1)} = \mathbf{x}^{(t)} - \gamma \nabla f(\mathbf{x}^{(t)})$, usually $\gamma \approx \frac{1}{t}$

\subsection*{SVD Thresholding}
$\mathbf{B}^{*}=\mathit{shrink}_\tau(\mathbf{A}):=\argmin_{\mathbf{B}}{\{\|\mathbf{A-B}\|^2_F + \tau\|\mathbf{B}\|_{*}\}}$\\
then with SVD holds $\mathbf{B^*=UD_\tau V^T, D_\tau} = \mathit{diag}(\max\{0,\sigma_i - \tau\})$,
$\Pi(\mathbf{X}) = x_{ij} \text{ if } (i,j) \in \mathcal{I} \text{ else } 0$ Iteration: $\mathbf{B}_{t+1}=\mathbf{B}_t + \eta_t \Pi(\mathbf{A} - \mathit{shrink}_\tau(\mathbf{B}_t))$
\subsection*{Lagrangian Multipliers}
Minimize  $f(\mathbf{x})$ s.t. $g_i(\mathbf{x}) \leq 0,\ i = 1, .., m$ (\textbf{inequality constr.}) and $h_i(\mathbf{x}) = \mathbf{a}_i^\top \mathbf{x} - b_i = 0,\ i = 1, .., p$ (\textbf{equality constraint})
\begin{compactdesc}
	\item[Lagrangian:] $L(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}) := f(\mathbf{x}) + \sum_{i=1}^m \lambda_i g_i(\mathbf{x}) + \sum_{i=1}^p \nu_i h_i(\mathbf{x})$
\end{compactdesc}

\subsection*{Convex Optimization}
Def. convex function: for $0 \leq \alpha \leq 1$: $f(\alpha \mathbf{x} + (1 - \alpha)\mathbf{y}) \leq \alpha f(\mathbf{x}) + (1-\alpha)f(\mathbf{y})$. local=global min, Def. convex set: for all x,y in set and t in [0,1], the point $(1-t)x + ty$ is also in set.\textbf{Convergence}: $f(\mathbf{x}^{(t)}) - f(\mathbf{x}^*) \le \frac{c}{t}$.
\textbf{Jensen's Inequality}: $f(tx_{1}+(1-t)x_{2})\leq tf(x_{1})+(1-t)f(x_{2}).$


\subsection*{Convex Relaxation}
Replace non-convex rank constraints by convex norm constraints.\\
Nuclear norm: $||A||_* = \sum_i \sigma_i$
